{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import backtrader as bt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common import env_checker\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "data = yf.download('AAPL', start='2020-01-01', end='2025-03-20', auto_adjust=False)\n",
    "data.reset_index(inplace=True)\n",
    "data['Date'] = pd.to_datetime(data['Date']) \n",
    "data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Open'] = data['Open'].astype('float32')\n",
    "data['High'] = data['High'].astype('float32')\n",
    "data['Low'] = data['Low'].astype('float32')\n",
    "data['Close'] = data['Close'].astype('float32')\n",
    "data['Volume'] = data['Volume'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, model_class, model_name, timesteps=10000):\n",
    "    model = model_class('MlpPolicy', env, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    model.save(f\"{model_name}_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    \"\"\"自訂的交易環境，用於強化學習模型訓練\"\"\"\n",
    "\n",
    "    def __init__(self, data, cash=10000, commission=0.001):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data.reset_index()\n",
    "        self.cash = cash\n",
    "        self.initial_cash = cash\n",
    "        self.commission = commission\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.Discrete(3)  # 0: 持有, 1: 買入, 2: 賣出\n",
    "\n",
    "        self.position = 0\n",
    "        self.net_worth = self.cash\n",
    "        self.prev_net_worth = self.cash\n",
    "\n",
    "        self.trades = []\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.array([\n",
    "            self.data.loc[self.current_step, 'Open'],\n",
    "            self.data.loc[self.current_step, 'High'],\n",
    "            self.data.loc[self.current_step, 'Low'],\n",
    "            self.data.loc[self.current_step, 'Close'],\n",
    "            self.data.loc[self.current_step, 'Volume'],\n",
    "        ], dtype=np.float32).flatten()  # ✅ 確保是 shape=(5,)\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.net_worth = self.cash\n",
    "        self.prev_net_worth = self.cash\n",
    "        self.trades = []\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data.loc[self.current_step, 'Close']\n",
    "        date = self.data.loc[self.current_step, 'Date']\n",
    "        self.trades.append({'Date': date, 'Action': action})\n",
    "\n",
    "        commission = 0\n",
    "\n",
    "        if action == 1:  # 買入\n",
    "            max_shares = int(self.cash / (current_price * (1 + self.commission)))\n",
    "            if max_shares > 0:\n",
    "                cost = max_shares * current_price * (1 + self.commission)\n",
    "                self.cash -= cost\n",
    "                self.position += max_shares\n",
    "                commission = cost * self.commission\n",
    "        elif action == 2:  # 賣出\n",
    "            if self.position > 0:\n",
    "                revenue = self.position * current_price * (1 - self.commission)\n",
    "                self.cash += revenue\n",
    "                commission = self.position * current_price * self.commission\n",
    "                self.position = 0\n",
    "\n",
    "        self.current_step += 1\n",
    "        self.net_worth = self.cash + self.position * current_price\n",
    "        reward = float(self.net_worth - self.prev_net_worth - commission)\n",
    "        self.prev_net_worth = self.net_worth\n",
    "\n",
    "        terminated = self.current_step >= len(self.data) - 1\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3524/2197746704.py:62: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  max_shares = int(self.cash / (current_price * (1 + self.commission)))\n",
      "/tmp/ipykernel_3524/2197746704.py:77: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  reward = float(self.net_worth - self.prev_net_worth - commission)\n"
     ]
    }
   ],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3524/2197746704.py:62: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  max_shares = int(self.cash / (current_price * (1 + self.commission)))\n",
      "/tmp/ipykernel_3524/2197746704.py:77: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  reward = float(self.net_worth - self.prev_net_worth - commission)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 207  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 202          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057431827 |\n",
      "|    clip_fraction        | 0.0909       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12e+05     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00364     |\n",
      "|    value_loss           | 3.02e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005456973 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.81e+04    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00121     |\n",
      "|    value_loss           | 1.5e+05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012627985 |\n",
      "|    clip_fraction        | 0.0222      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.72e+05    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00402    |\n",
      "|    value_loss           | 2.26e+05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 204         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003328596 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.36e+05    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.000232    |\n",
      "|    value_loss           | 3.1e+05     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f22ff5813f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset()\n",
    "for i in range(len(data) - 1):\n",
    "    obs = env.envs[0]._get_obs()\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, truncated, info = env.envs[0].step(action)\n",
    "    if dones:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
